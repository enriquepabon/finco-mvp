# Implementaci√≥n de OpenAI Whisper para Reconocimiento de Voz

## ‚úÖ Completado

1. **Endpoint de transcripci√≥n**: `/api/transcribe-audio`
   - Recibe audio en formato webm/mp4
   - Usa OpenAI Whisper (model: whisper-1)
   - Transcribe a espa√±ol
   - Maneja errores espec√≠ficos de OpenAI

## üöß Pendiente: Actualizar `VoiceTransactionModal.tsx`

El componente `VoiceTransactionModal.tsx` actualmente usa Web Speech API (que est√° fallando con error `network`).

Necesitamos reemplazarlo con **MediaRecorder** + **OpenAI Whisper**.

---

## Cambios necesarios en `VoiceTransactionModal.tsx`:

### 1. Eliminar referencias a SpeechRecognition

**Buscar y eliminar:**
```typescript
// L√≠neas 14-60: Interfaces de SpeechRecognition
interface SpeechRecognitionAlternative { ... }
interface SpeechRecognitionResult { ... }
interface SpeechRecognitionResultList { ... }
interface SpeechRecognitionEvent { ... }
interface SpeechRecognitionErrorEvent { ... }
interface SpeechRecognition { ... }
type SpeechRecognitionConstructor = ...
interface WindowWithSpeechRecognition { ... }
```

### 2. Cambiar referencias en el componente

**L√≠neas 100-102** - Cambiar:
```typescript
const recognitionRef = useRef<SpeechRecognition | null>(null);
const retryCountRef = useRef<number>(0);
const MAX_RETRIES = 3;
```

**Por:**
```typescript
const mediaRecorderRef = useRef<MediaRecorder | null>(null);
const audioChunksRef = useRef<Blob[]>([]);
const streamRef = useRef<MediaStream | null>(null);
```

### 3. Reemplazar useEffect (l√≠neas 113-193)

**Eliminar todo el useEffect que inicializa Speech Recognition.**

**Reemplazar con:**
```typescript
// Cleanup: Detener grabaci√≥n al cerrar el modal
useEffect(() => {
  return () => {
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
    }
  };
}, []);
```

### 4. Reemplazar funci√≥n `startRecording` (l√≠neas 195-237)

**Eliminar la funci√≥n actual.**

**Reemplazar con:**
```typescript
const startRecording = async () => {
  try {
    setError('');
    setTranscript('');
    setParsedData(null);
    audioChunksRef.current = [];

    console.log('üé§ Solicitando acceso al micr√≥fono...');

    // Solicitar acceso al micr√≥fono
    const stream = await navigator.mediaDevices.getUserMedia({ 
      audio: {
        echoCancellation: true,
        noiseSuppression: true,
        sampleRate: 44100,
      } 
    });

    streamRef.current = stream;

    // Crear MediaRecorder
    const mimeType = MediaRecorder.isTypeSupported('audio/webm') 
      ? 'audio/webm' 
      : 'audio/mp4';

    const mediaRecorder = new MediaRecorder(stream, { mimeType });
    mediaRecorderRef.current = mediaRecorder;

    // Capturar chunks de audio
    mediaRecorder.ondataavailable = (event) => {
      if (event.data.size > 0) {
        audioChunksRef.current.push(event.data);
      }
    };

    // Al terminar la grabaci√≥n, transcribir
    mediaRecorder.onstop = async () => {
      console.log('üõë Grabaci√≥n detenida, transcribiendo...');
      await transcribeAudio();
    };

    // Iniciar grabaci√≥n
    mediaRecorder.start();
    setIsRecording(true);
    console.log('‚úÖ Grabaci√≥n iniciada');

  } catch (error: any) {
    console.error('‚ùå Error al iniciar grabaci√≥n:', error);
    
    if (error.name === 'NotAllowedError') {
      setError('üé§ Permisos de micr√≥fono denegados.\n\nPor favor:\n1. Click en el √≠cono üîí en la barra de direcci√≥n\n2. Permite el acceso al micr√≥fono\n3. Recarga la p√°gina e intenta de nuevo');
    } else if (error.name === 'NotFoundError') {
      setError('üéôÔ∏è No se detect√≥ ning√∫n micr√≥fono.\n\nVerifica que tu micr√≥fono est√© conectado correctamente.');
    } else {
      setError(`Error al acceder al micr√≥fono: ${error.message}`);
    }
    setIsRecording(false);
  }
};
```

### 5. Eliminar funci√≥n `attemptMicrophoneAccess` (l√≠neas 239-264)

**Esta funci√≥n ya no es necesaria.**

### 6. Reemplazar funci√≥n `stopRecording` (l√≠neas 266-270)

**Cambiar:**
```typescript
const stopRecording = () => {
  if (recognitionRef.current && isRecording) {
    recognitionRef.current.stop();
  }
};
```

**Por:**
```typescript
const stopRecording = () => {
  if (mediaRecorderRef.current && mediaRecorderRef.current.state === 'recording') {
    console.log('‚èπÔ∏è Deteniendo grabaci√≥n...');
    mediaRecorderRef.current.stop();
    setIsRecording(false);

    // Detener el stream
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
    }
  }
};
```

### 7. Agregar nueva funci√≥n `transcribeAudio`

**Agregar despu√©s de `stopRecording`:**
```typescript
const transcribeAudio = async () => {
  try {
    setLoading(true);
    setError('');

    // Crear blob de audio
    const audioBlob = new Blob(audioChunksRef.current, { 
      type: audioChunksRef.current[0]?.type || 'audio/webm' 
    });

    console.log('üì¶ Audio blob creado:', {
      size: audioBlob.size,
      type: audioBlob.type,
    });

    if (audioBlob.size === 0) {
      setError('No se captur√≥ audio. Intenta hablar m√°s fuerte o cerca del micr√≥fono.');
      setLoading(false);
      return;
    }

    // Enviar a API de transcripci√≥n
    const formData = new FormData();
    formData.append('audio', audioBlob, 'audio.webm');

    console.log('üöÄ Enviando audio a Whisper...');

    const response = await fetch('/api/transcribe-audio', {
      method: 'POST',
      body: formData,
    });

    const data = await response.json();

    if (!response.ok) {
      throw new Error(data.error || 'Error al transcribir audio');
    }

    console.log('‚úÖ Transcripci√≥n exitosa:', data.text);
    setTranscript(data.text);

    // Auto-procesar con IA
    await processWithAI(data.text);

  } catch (error: any) {
    console.error('‚ùå Error en transcripci√≥n:', error);
    setError(`Error al transcribir audio: ${error.message}`);
  } finally {
    setLoading(false);
  }
};
```

---

## üîÑ Resumen de cambios

| Antes (Web Speech API) | Despu√©s (MediaRecorder + Whisper) |
|------------------------|-------------------------------------|
| `SpeechRecognition` | `MediaRecorder` |
| `recognitionRef` | `mediaRecorderRef` + `audioChunksRef` + `streamRef` |
| Transcripci√≥n en tiempo real | Transcripci√≥n al detener grabaci√≥n |
| Dependencia de servicios Google | API propia con OpenAI Whisper |
| Error `network` persistente | ‚úÖ Funciona en cualquier navegador |

---

## üéØ Ventajas de la nueva implementaci√≥n

1. **‚úÖ M√°s confiable**: No depende de servicios externos del navegador
2. **‚úÖ Mejor calidad**: Whisper es muy preciso en espa√±ol
3. **‚úÖ Control total**: Manejamos todo el flujo en nuestro backend
4. **‚úÖ Sin errores de red**: No m√°s `network` errors
5. **‚úÖ Funciona en todos los navegadores**: Chrome, Edge, Arc, Safari

---

## üí∞ Costo

- **OpenAI Whisper**: $0.006 por minuto de audio
- **Promedio**: Una transacci√≥n por voz = 5-10 segundos = $0.0005 - $0.001
- **1,000 transacciones**: ~$0.50 - $1.00

---

## üöÄ Pr√≥ximos pasos

1. Aplicar los cambios en `VoiceTransactionModal.tsx`
2. Commit y push
3. Probar en producci√≥n

---

**√öltima actualizaci√≥n**: 2025-11-10
**Estado**: ‚úÖ Endpoint listo, pendiente actualizaci√≥n del componente

